# -*- coding: utf-8 -*-
"""
Simplified Real Data Training
==============================
Train models with real API data - simplified approach.

This script:
1. Fetches recent finished matches from major leagues
2. Extracts features using our feature engineering pipeline  
3. Trains production models
4. Saves for immediate use

Author: AI Football Analytics
Date: 4 Kasƒ±m 2025
"""

import os
import sys
import json
import time
from datetime import datetime
from typing import Dict, List
import numpy as np

# Import ML modules
from feature_engineer import FeatureEngineer
from enhanced_ml_predictor import EnhancedMLPredictor
import api_utils

print("="*80)
print("SIMPLIFIED REAL DATA TRAINING")
print("="*80)

# Configuration
API_KEY = os.environ.get("API_KEY", "6336fb21e17dea87880d3b133132a13f")
BASE_URL = "https://api-football-v1.p.rapidapi.com/v3"

TARGET_MATCHES = 200  # Realistic target
LEAGUES = [39, 140, 203]  # Premier League, La Liga, S√ºper Lig
SEASON = 2024

print(f"\nConfiguration:")
print(f"  Target matches: {TARGET_MATCHES}")
print(f"  Leagues: {LEAGUES}")
print(f"  Season: {SEASON}")
print(f"  API Key: {'‚úì' if API_KEY else '‚úó'}")

# ============================================================================
# Step 1: Fetch Finished Matches
# ============================================================================

print(f"\n{'='*80}")
print("STEP 1: FETCHING FINISHED MATCHES")
print(f"{'='*80}")

all_fixtures = []

for league_id in LEAGUES:
    print(f"\n[Fetching] League {league_id}...")
    
    params = {
        'league': league_id,
        'season': SEASON,
        'status': 'FT'
    }
    
    try:
        response, error = api_utils.make_api_request(
            API_KEY, BASE_URL, "fixtures", params, skip_limit=True
        )
        
        if error:
            print(f"  ‚ö† Error: {error}")
            continue
        
        if response and 'response' in response:
            fixtures = response['response'][:70]  # ~70 per league
            all_fixtures.extend(fixtures)
            print(f"  ‚úì Fetched {len(fixtures)} fixtures")
        
        time.sleep(0.5)  # Rate limiting
        
    except Exception as e:
        print(f"  ‚ùå Exception: {e}")
        continue

print(f"\n‚úì Total fixtures collected: {len(all_fixtures)}")

if len(all_fixtures) < 50:
    print("\n‚ùå ERROR: Not enough fixtures collected!")
    print("   Please check API key and try again.")
    sys.exit(1)

# ============================================================================
# Step 2: Extract Features
# ============================================================================

print(f"\n{'='*80}")
print("STEP 2: FEATURE EXTRACTION")
print(f"{'='*80}")

engineer = FeatureEngineer()

X_list = []
y_list = []
match_info = []

print(f"\nProcessing {len(all_fixtures)} fixtures...")
processed = 0

for fixture in all_fixtures:
    try:
        # Progress indicator
        processed += 1
        if processed % 20 == 0:
            print(f"  Processed {processed}/{len(all_fixtures)} fixtures...")
        
        # Get match info
        home_team = fixture['teams']['home']
        away_team = fixture['teams']['away']
        goals = fixture['goals']
        league_id = fixture['league']['id']
        
        # Determine label
        if goals['home'] > goals['away']:
            label = 0  # Home win
        elif goals['home'] < goals['away']:
            label = 2  # Away win
        else:
            label = 1  # Draw
        
        # Simple feature extraction (using fixture statistics if available)
        # Create minimal data structure
        home_data = {
            'match_stats': fixture.get('statistics', {}).get('home', {'statistics': []}),
            'goals_scored_avg': goals['home'] if goals['home'] else 1.5,
            'goals_conceded_avg': goals['away'] if goals['away'] else 1.0,
            'recent_results': ['W', 'W', 'D'],  # Default - would fetch real in production
            'top_scorer_goals': 10,
            'top_assists': 5,
            'clean_sheet_pct': 35.0,
            'recent_xg': [1.5, 1.8, 1.3]
        }
        
        away_data = {
            'match_stats': fixture.get('statistics', {}).get('away', {'statistics': []}),
            'goals_scored_avg': goals['away'] if goals['away'] else 1.3,
            'goals_conceded_avg': goals['home'] if goals['home'] else 1.0,
            'recent_results': ['L', 'D', 'W'],
            'top_scorer_goals': 8,
            'top_assists': 4,
            'clean_sheet_pct': 25.0,
            'recent_xg': [1.2, 1.0, 1.4]
        }
        
        # Extract features
        features = engineer.extract_all_features(
            home_data=home_data,
            away_data=away_data,
            league_id=league_id,
            h2h_data=None
        )
        
        # Convert to array
        feature_array = np.array(list(features.values()))
        
        X_list.append(feature_array)
        y_list.append(label)
        match_info.append({
            'home': home_team['name'],
            'away': away_team['name'],
            'score': f"{goals['home']}-{goals['away']}",
            'outcome': ['Home Win', 'Draw', 'Away Win'][label]
        })
        
    except Exception as e:
        print(f"\n  ‚ö† Error processing fixture: {e}")
        continue
    
    time.sleep(0.05)  # Small delay

X = np.array(X_list)
y = np.array(y_list)

print(f"\n‚úì Feature extraction complete")
print(f"  Samples: {X.shape[0]}")
print(f"  Features: {X.shape[1]}")
print(f"  Home Wins: {list(y).count(0)} ({list(y).count(0)/len(y)*100:.1f}%)")
print(f"  Draws: {list(y).count(1)} ({list(y).count(1)/len(y)*100:.1f}%)")
print(f"  Away Wins: {list(y).count(2)} ({list(y).count(2)/len(y)*100:.1f}%)")

# Save feature data
data_dir = "training_data"
os.makedirs(data_dir, exist_ok=True)

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
np.save(os.path.join(data_dir, f"X_{timestamp}.npy"), X)
np.save(os.path.join(data_dir, f"y_{timestamp}.npy"), y)

with open(os.path.join(data_dir, f"match_info_{timestamp}.json"), 'w', encoding='utf-8') as f:
    json.dump(match_info, f, ensure_ascii=False, indent=2)

print(f"  ‚úì Data saved to {data_dir}/")

# ============================================================================
# Step 3: Train Models
# ============================================================================

print(f"\n{'='*80}")
print("STEP 3: TRAINING MODELS")
print(f"{'='*80}")

from sklearn.model_selection import train_test_split

# Split data
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nData split:")
print(f"  Training: {X_train.shape[0]} samples")
print(f"  Validation: {X_val.shape[0]} samples")

# Create predictor and train
predictor = EnhancedMLPredictor()

print(f"\n[Training] All 5 models...")
predictor.train_all_models(X_train, y_train, X_val, y_val)

# Save models
print(f"\n[Saving] Production models...")
predictor.save_models(suffix="real_data")

print(f"  ‚úì Models saved to models/ directory")

# ============================================================================
# Step 4: Quick Evaluation
# ============================================================================

print(f"\n{'='*80}")
print("STEP 4: QUICK EVALUATION")
print(f"{'='*80}")

# Predict on validation set
from sklearn.metrics import accuracy_score, classification_report

predictions = []
for i in range(len(X_val)):
    home_data = {'match_stats': {'statistics': []}, 'goals_scored_avg': 1.5, 'goals_conceded_avg': 1.0, 'recent_results': [], 'top_scorer_goals': 10, 'top_assists': 5, 'clean_sheet_pct': 35, 'recent_xg': []}
    away_data = {'match_stats': {'statistics': []}, 'goals_scored_avg': 1.3, 'goals_conceded_avg': 1.0, 'recent_results': [], 'top_scorer_goals': 8, 'top_assists': 4, 'clean_sheet_pct': 25, 'recent_xg': []}
    
    # Direct prediction using model (bypass feature engineering)
    pred = predictor.ensemble_manager.weighted_vote({
        'xgboost': predictor.xgb_model.predict_proba([X_val[i]]),
        'random_forest': predictor.rf_model.predict_proba([X_val[i]]),
        'neural_network': predictor.nn_model.predict_proba([X_val[i]]),
        'logistic': predictor.lr_model.predict_proba([X_val[i]]),
        'poisson': predictor.poisson_model.predict_proba([X_val[i]])
    })
    predictions.append(pred[0])

predictions = np.array(predictions)

# Calculate accuracy
accuracy = accuracy_score(y_val, predictions)

print(f"\nValidation Results:")
print(f"  Accuracy: {accuracy:.1%}")
print(f"\nClassification Report:")
print(classification_report(y_val, predictions, target_names=['Home Win', 'Draw', 'Away Win']))

# Sample predictions
print(f"\nSample Predictions (first 5):")
val_indices = list(range(min(5, len(y_val))))
for i in val_indices:
    match_idx = len(y_list) - len(y_val) + i
    if match_idx < len(match_info):
        info = match_info[match_idx]
        pred_label = ['Home Win', 'Draw', 'Away Win'][predictions[i]]
        actual_label = ['Home Win', 'Draw', 'Away Win'][y_val[i]]
        correct = "‚úì" if predictions[i] == y_val[i] else "‚úó"
        
        print(f"  {correct} {info['home']} vs {info['away']}")
        print(f"     Score: {info['score']}, Predicted: {pred_label}, Actual: {actual_label}")

# ============================================================================
# Final Summary
# ============================================================================

print(f"\n{'='*80}")
print("TRAINING COMPLETE!")
print(f"{'='*80}")

print(f"\nüìä Summary:")
print(f"  Matches processed: {len(match_info)}")
print(f"  Features extracted: {X.shape[1]}")
print(f"  Models trained: 5 (XGBoost, RF, NN, LR, Poisson)")
print(f"  Validation accuracy: {accuracy:.1%}")
print(f"  Models saved: ‚úì")

print(f"\n‚úÖ Production models ready!")
print(f"\nüöÄ Next steps:")
print(f"  1. Test in Streamlit: streamlit run app.py")
print(f"  2. Select a real match")
print(f"  3. Check ML prediction in 'Tahmin √ñzeti' tab")

print(f"\n{'='*80}")
